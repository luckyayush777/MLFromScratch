#My Accuracy is stuck at 81
For the purposes of an optimization harness, I need a set of non trivial models first. Currently I use a linear classifier which is just ( weights, bias ) * features and then backprop on this, this gets me to 81% on fashion MNIST. The proper solution to this problem is an image network but I need an intermediate step first and that happens to be more layers. 
These layers do what we call representation learning ( in more layers than the 2 I plan on using ), which is to say that they do the hard job of identifying features. Historically this would have been annoying. This is essentially finding out the special edges or shapes in the early layers and moving up in the abstraction layer in the later layers. People theorize ( but are not sure ) as to what exactly each layer represents ( as an example at what layer does the model learn that I am seeing a 9 now) , perhaps this is not even useful as long as we get to our accuracy number.

Weirdly enough I don't even beat my one layer linear classifier ( both stall at 81% ). This is not even close to the ceiling for something like this on a MLP. I need to think about shuffling the data or perhaps data augmentation but that seems like a weird approach as this is not the natural solution for a problem like this. I am imagining that augmentation will work well with a problem and solution synergy. Let me try shuffling the data so it's never loaded in the exact same order.
